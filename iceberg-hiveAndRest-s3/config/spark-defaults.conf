# Enable Iceberg SQL extensions
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

# Default catalog is REST catalog (backed by minio via REST fixture)
spark.sql.defaultCatalog=rest

# 1) REST Catalog (already provided by iceberg-rest service)
spark.sql.catalog.rest=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.rest.type=rest
spark.sql.catalog.rest.uri=http://iceberg-rest:8181
spark.sql.catalog.rest.io-impl=org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.rest.warehouse=s3://warehouse/
spark.sql.catalog.rest.client.region=us-east-1
spark.sql.catalog.rest.client.aws.access-key-id=admin
spark.sql.catalog.rest.client.aws.secret-access-key=password
spark.sql.catalog.rest.client.s3.endpoint=http://minio:9000
spark.sql.catalog.rest.client.s3.path-style-access=true

# 2) Hive Catalog (HMS on postgres)
spark.sql.catalog.hive=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.hive.type=hive
spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083
spark.sql.catalog.hive.warehouse=hdfs://mycluster/iceberg/hive/warehouse

# 3) Hadoop Catalog (store metadata directly in HDFS)
spark.sql.catalog.hadoop=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.hadoop.type=hadoop
# Use HA nameservice exposed by hadoop-ha compose
spark.sql.catalog.hadoop.warehouse=hdfs://mycluster/iceberg/warehouse

# Optional: Spark warehouse dir for Spark SQL (not used by Iceberg tables)
spark.sql.warehouse.dir=/home/iceberg/warehouse

# Quicker local dev
spark.ui.prometheus.enabled=false